# Viora: Advancing Mathematical and Multimodal Reasoning in AI

Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit remarkable problem-solving skills across a wide range of tasks and domains. However, their proficiency in mathematical reasoning within visual contexts remains underexplored. Addressing this gap, we introduce Viora comprehensive benchmark crafted to evaluate and enhance the mathematical and visual reasoning capabilities of AI systems. Viore integrates challenges from diverse sources, comprising 6,141 examples from 28 existing multimodal datasets centered around mathematics, alongside three newly curated datasets: IQTest, FunctionQA, and PaperQA. These tasks demand intricate, fine-grained visual understanding and advanced compositional reasoning, pushing the boundaries of current state-of-the-art foundation models.

Viora not only highlights existing limitations but also serves as a stepping stone to improve AI's mathematical reasoning in real-world visual contexts. By fostering progress in this domain, we aim to advance AI systems that can seamlessly integrate mathematical and visual problem-solving skills, unlocking new applications in education, research, and complex decision-making tasks.




#  Accuracies of one leading LLM 



![image](https://github.com/user-attachments/assets/242fa645-4158-4fad-95da-f8279206afe4)

  Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance on Viora.


# ðŸ§  Related Work
Explore our additional research on large language models and large multimodal models , focusing on mathematical reasoning, scientific reasoning, and multimodal reasoning:

. [LLaMA-Adapter V2]            [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://github.com/OpenGVLab/LLaMA-Adapter)

. [LLaMA-Adapter]         [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://github.com/OpenGVLab/LLaMA-Adapter)

. [Chameleon]             [Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2307.10635)

. [ScienceQA]             [Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering](https://scienceqa.github.io/)

. [TheoremQA]             [TheoremQA: A Theorem-driven Question Answering dataset](https://arxiv.org/abs/2305.12524)

. [Inter-GPS]             [Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning](https://lupantech.github.io/inter-gps/)

. [PromptPG]              [Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning](https://promptpg.github.io/)

. [SciBench]              [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2307.10635)

. [DL4MATH]               [A Survey of Deep Learning for Mathematical Reasoning](https://arxiv.org/abs/2212.10535)

. [IconQA]                [IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning](https://iconqa.github.io/)

. [LÄ«la]                  [A Unified Benchmark for Mathematical Reasoning](https://lila.apps.allenai.org/)
